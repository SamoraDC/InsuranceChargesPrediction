#!/usr/bin/env python3
"""
ComparaÃ§Ã£o de Performance: Modelo Local vs Deploy
MÃ©tricas: RÂ², MAE, MSE, RMSE, MAPE, MBE
USANDO DADOS REAIS DO INSURANCE.CSV
"""

import numpy as np
import pandas as pd
import sys
from pathlib import Path
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import train_test_split
import warnings
warnings.filterwarnings('ignore')

# Add paths
sys.path.insert(0, str(Path('.') / 'src'))

def calculate_metrics(y_true, y_pred):
    """Calcula todas as mÃ©tricas de performance."""
    
    # Converter para numpy arrays
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # RÂ² (R-squared)
    r2 = r2_score(y_true, y_pred)
    
    # MAE (Mean Absolute Error)
    mae = mean_absolute_error(y_true, y_pred)
    
    # MSE (Mean Squared Error)
    mse = mean_squared_error(y_true, y_pred)
    
    # RMSE (Root Mean Squared Error)
    rmse = np.sqrt(mse)
    
    # MAPE (Mean Absolute Percentage Error)
    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100
    
    # MBE (Mean Bias Error)
    mbe = np.mean(y_pred - y_true)
    
    return {
        'RÂ²': r2,
        'MAE': mae,
        'MSE': mse,
        'RMSE': rmse,
        'MAPE': mape,
        'MBE': mbe
    }

def load_real_data():
    """Carrega os dados reais do insurance.csv"""
    
    print("ğŸ“‚ Carregando dados reais do insurance.csv...")
    
    # Tentar diferentes caminhos
    possible_paths = [
        "data/insurance.csv",
        "data/raw/insurance.csv", 
        "../data/insurance.csv"
    ]
    
    for path in possible_paths:
        try:
            df = pd.read_csv(path)
            print(f"âœ… Dados carregados de: {path}")
            print(f"ğŸ“Š Shape: {df.shape}")
            print(f"ğŸ“‹ Colunas: {list(df.columns)}")
            return df
        except Exception as e:
            continue
    
    raise FileNotFoundError("âŒ Arquivo insurance.csv nÃ£o encontrado!")

def prepare_test_data(df, test_size=0.3):
    """Prepara dados de teste a partir do dataset real."""
    
    print(f"ğŸ”„ Preparando dados de teste (tamanho do teste: {test_size})")
    
    # Separar features e target
    features = ['age', 'sex', 'bmi', 'children', 'smoker', 'region']
    target = 'charges'
    
    X = df[features]
    y = df[target]
    
    # Split dos dados
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=test_size, random_state=42, stratify=df['smoker']
    )
    
    print(f"ğŸ“ˆ Dados de treino: {len(X_train)} amostras")
    print(f"ğŸ“Š Dados de teste: {len(X_test)} amostras")
    
    # Converter para formato adequado
    test_data = []
    for idx, row in X_test.iterrows():
        test_data.append({
            'age': int(row['age']),
            'sex': row['sex'],
            'bmi': float(row['bmi']),
            'children': int(row['children']),
            'smoker': row['smoker'],
            'region': row['region'],
            'true_charges': float(y_test.iloc[X_test.index.get_loc(idx)])
        })
    
    return test_data

def test_local_model(test_data):
    """Testa o modelo local."""
    
    print("ğŸ  Testando Modelo Local...")
    
    try:
        from insurance_prediction.models.predictor import predict_insurance_premium
        
        predictions = []
        true_values = []
        errors = 0
        
        for i, data in enumerate(test_data):
            try:
                result = predict_insurance_premium(
                    age=data['age'],
                    sex=data['sex'],
                    bmi=data['bmi'],
                    children=data['children'],
                    smoker=data['smoker'],
                    region=data['region']
                )
                
                predictions.append(result['predicted_premium'])
                true_values.append(data['true_charges'])
                
            except Exception as e:
                errors += 1
                if errors <= 3:  # Mostra apenas os primeiros 3 erros
                    print(f"Erro na prediÃ§Ã£o local {i+1}: {e}")
                continue
        
        print(f"âœ… Modelo Local: {len(predictions)}/{len(test_data)} prediÃ§Ãµes (erros: {errors})")
        return true_values, predictions
        
    except Exception as e:
        print(f"âŒ Erro no modelo local: {e}")
        return [], []

def test_deploy_model(test_data):
    """Testa o modelo de deploy."""
    
    print("â˜ï¸ Testando Modelo Deploy...")
    
    try:
        from deploy.model_utils import load_model, predict_premium
        
        # Carregar modelo uma vez
        model_data = load_model()
        print(f"ğŸ”§ Modelo carregado: {type(model_data['model']).__name__}")
        print(f"ğŸ¯ Features: {model_data.get('feature_names', 'N/A')}")
        
        predictions = []
        true_values = []
        errors = 0
        
        for i, data in enumerate(test_data):
            try:
                input_data = {
                    'age': data['age'],
                    'sex': data['sex'],
                    'bmi': data['bmi'],
                    'children': data['children'],
                    'smoker': data['smoker'],
                    'region': data['region']
                }
                
                result = predict_premium(input_data, model_data)
                
                if result['success']:
                    predictions.append(result['predicted_premium'])
                    true_values.append(data['true_charges'])
                else:
                    errors += 1
                    if errors <= 3:
                        print(f"Erro na prediÃ§Ã£o deploy {i+1}: {result.get('error', 'Desconhecido')}")
                
            except Exception as e:
                errors += 1
                if errors <= 3:
                    print(f"Erro na prediÃ§Ã£o deploy {i+1}: {e}")
                continue
        
        print(f"âœ… Modelo Deploy: {len(predictions)}/{len(test_data)} prediÃ§Ãµes (erros: {errors})")
        return true_values, predictions
        
    except Exception as e:
        print(f"âŒ Erro no modelo deploy: {e}")
        return [], []

def analyze_data_distribution(df):
    """Analisa a distribuiÃ§Ã£o dos dados reais."""
    
    print("\nğŸ“Š ANÃLISE DOS DADOS REAIS:")
    print("=" * 50)
    
    print(f"Total de amostras: {len(df)}")
    print(f"Charges mÃ©dio: ${df['charges'].mean():,.2f}")
    print(f"Charges mediano: ${df['charges'].median():,.2f}")
    print(f"Charges mÃ­n: ${df['charges'].min():,.2f}")
    print(f"Charges mÃ¡x: ${df['charges'].max():,.2f}")
    print(f"Desvio padrÃ£o: ${df['charges'].std():,.2f}")
    
    print("\nğŸš­ DistribuiÃ§Ã£o por fumante:")
    smoker_stats = df.groupby('smoker')['charges'].agg(['count', 'mean', 'std'])
    print(smoker_stats)
    
    print("\nğŸ‘¥ DistribuiÃ§Ã£o por sexo:")
    sex_stats = df.groupby('sex')['charges'].agg(['count', 'mean', 'std'])
    print(sex_stats)
    
    print("\nğŸ—ºï¸ DistribuiÃ§Ã£o por regiÃ£o:")
    region_stats = df.groupby('region')['charges'].agg(['count', 'mean', 'std'])
    print(region_stats)

def compare_models():
    """Compara a performance dos dois modelos usando dados REAIS."""
    
    print("ğŸ” COMPARAÃ‡ÃƒO DE PERFORMANCE: LOCAL vs DEPLOY")
    print("ğŸ“„ USANDO DADOS REAIS DO INSURANCE.CSV")
    print("=" * 60)
    
    # Carregar dados reais
    df = load_real_data()
    
    # Analisar distribuiÃ§Ã£o dos dados
    analyze_data_distribution(df)
    
    # Preparar dados de teste
    print("\n" + "="*50)
    test_data = prepare_test_data(df, test_size=0.2)  # 20% para teste
    
    # Testar modelo local
    print("\n" + "="*40)
    true_local, pred_local = test_local_model(test_data)
    
    # Testar modelo deploy
    print("\n" + "="*40)
    true_deploy, pred_deploy = test_deploy_model(test_data)
    
    # Calcular mÃ©tricas se ambos funcionaram
    if len(pred_local) > 0 and len(pred_deploy) > 0:
        print("\n" + "="*60)
        print("ğŸ“ˆ RESULTADOS DA COMPARAÃ‡ÃƒO COM DADOS REAIS")
        print("="*60)
        
        # Calcular mÃ©tricas para cada modelo
        metrics_local = calculate_metrics(true_local, pred_local)
        metrics_deploy = calculate_metrics(true_deploy, pred_deploy)
        
        # Exibir resultados
        print(f"\nğŸ  MODELO LOCAL ({len(pred_local)} amostras):")
        print("-" * 40)
        for metric, value in metrics_local.items():
            if metric == 'RÂ²':
                print(f"{metric:>8}: {value:>12.4f}")
            elif metric in ['MAPE']:
                print(f"{metric:>8}: {value:>11.2f}%")
            else:
                print(f"{metric:>8}: ${value:>11,.2f}")
        
        print(f"\nâ˜ï¸ MODELO DEPLOY ({len(pred_deploy)} amostras):")
        print("-" * 40)
        for metric, value in metrics_deploy.items():
            if metric == 'RÂ²':
                print(f"{metric:>8}: {value:>12.4f}")
            elif metric in ['MAPE']:
                print(f"{metric:>8}: {value:>11.2f}%")
            else:
                print(f"{metric:>8}: ${value:>11,.2f}")
        
        # ComparaÃ§Ã£o direta
        print(f"\nğŸ† COMPARAÃ‡ÃƒO DIRETA:")
        print("-" * 60)
        print(f"{'MÃ©trica':>10} | {'Local':>12} | {'Deploy':>12} | {'Melhor':>12}")
        print("-" * 60)
        
        for metric in metrics_local.keys():
            local_val = metrics_local[metric]
            deploy_val = metrics_deploy[metric]
            
            # Determinar qual Ã© melhor (maior RÂ², menor para outras)
            if metric == 'RÂ²':
                better = 'LOCAL' if local_val > deploy_val else 'DEPLOY'
                if abs(local_val - deploy_val) < 0.001:
                    better = 'EMPATE'
            else:
                better = 'LOCAL' if local_val < deploy_val else 'DEPLOY'
                if abs(local_val - deploy_val) < 1:
                    better = 'EMPATE'
            
            if metric == 'RÂ²':
                print(f"{metric:>10} | {local_val:>12.4f} | {deploy_val:>12.4f} | {better:>12}")
            elif metric == 'MAPE':
                print(f"{metric:>10} | {local_val:>11.2f}% | {deploy_val:>11.2f}% | {better:>12}")
            else:
                print(f"{metric:>10} | ${local_val:>11,.0f} | ${deploy_val:>11,.0f} | {better:>12}")
        
        # Score geral
        print("\nğŸ¯ SCORE GERAL:")
        print("-" * 40)
        
        local_wins = 0
        deploy_wins = 0
        ties = 0
        
        for metric in metrics_local.keys():
            if metric == 'RÂ²':
                if metrics_local[metric] > metrics_deploy[metric] + 0.001:
                    local_wins += 1
                elif metrics_deploy[metric] > metrics_local[metric] + 0.001:
                    deploy_wins += 1
                else:
                    ties += 1
            else:
                if metrics_local[metric] < metrics_deploy[metric] - 1:
                    local_wins += 1
                elif metrics_deploy[metric] < metrics_local[metric] - 1:
                    deploy_wins += 1
                else:
                    ties += 1
        
        print(f"Local ganha em: {local_wins}/6 mÃ©tricas")
        print(f"Deploy ganha em: {deploy_wins}/6 mÃ©tricas")
        print(f"Empates: {ties}/6 mÃ©tricas")
        
        if local_wins > deploy_wins:
            print("ğŸ† VENCEDOR: MODELO LOCAL")
        elif deploy_wins > local_wins:
            print("ğŸ† VENCEDOR: MODELO DEPLOY")
        else:
            print("ğŸ¤ EMPATE")
        
        # AnÃ¡lise detalhada
        print(f"\nğŸ” ANÃLISE DETALHADA:")
        print("-" * 40)
        print(f"DiferenÃ§a RÂ²: {abs(metrics_local['RÂ²'] - metrics_deploy['RÂ²']):.4f}")
        print(f"DiferenÃ§a MAE: ${abs(metrics_local['MAE'] - metrics_deploy['MAE']):,.2f}")
        print(f"DiferenÃ§a RMSE: ${abs(metrics_local['RMSE'] - metrics_deploy['RMSE']):,.2f}")
        
        # RecomendaÃ§Ãµes
        if deploy_wins > local_wins:
            print("\nğŸ’¡ RECOMENDAÃ‡ÃƒO: Use o modelo DEPLOY em produÃ§Ã£o")
        elif local_wins > deploy_wins:
            print("\nğŸ’¡ RECOMENDAÃ‡ÃƒO: Considere atualizar o modelo DEPLOY com o modelo LOCAL")
        else:
            print("\nğŸ’¡ RECOMENDAÃ‡ÃƒO: Ambos modelos tÃªm performance similar")
        
    else:
        print("âŒ NÃ£o foi possÃ­vel comparar - um ou ambos modelos falharam")
        if len(pred_local) == 0:
            print("   - Modelo LOCAL falhou")
        if len(pred_deploy) == 0:
            print("   - Modelo DEPLOY falhou")

if __name__ == "__main__":
    compare_models() 